{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/k2moon/ml-hg/blob/main/hg_05_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp6fW8MP-mrO"
      },
      "source": [
        "# 트리의 앙상블\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 핵심 키워드\n",
        "---\n",
        "- 앙상블 학습\n",
        "- 랜덤 포레스트\n",
        "- 엑스트라 트리\n",
        "- 그레이던트 부스팅"
      ],
      "metadata": {
        "id": "1vOTRbhz_DnF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv1IwHmU-mrU"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/5-3.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />구글 코랩에서 실행하기</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 앙상블 학습 ensemble learning\n",
        "---\n",
        "- 더 좋은 예측 결과를 만들기 위해 여러개의 모델을 훈련하는 머신러닝 알고리즘\n",
        "- 정형 데이터 structured data 에서 띄어난 성과를 보임\n",
        "- 비정형 데이터 structured data는 신경망 알고리즘 사용"
      ],
      "metadata": {
        "id": "lfwkguPR_fEA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIaIAizcRSG-"
      },
      "source": [
        "## 랜덤 포레스트 Random Forest\n",
        "---\n",
        "- 대표적인 결정 트리 기반의 앙상블 학습 방법\n",
        "- 여러 개의 결정 트리(Decision Tree)를 결합하여 하나의 결정 트리보다 더 좋은 성능을 내는 머신러닝 기법\n",
        "- 여러 개의 약 분류기 (Weak Classifier)를 결합하여 강 분류기(Strong Classifier)를 만드는 것\n",
        "- 부트스트랩 샘플을 사용하고 랜덤하게 일부 특성을 선택하여 트리를 만드는 것이 특징\n",
        "- 사이킷런의 랜덤 포레스트는 기본적으로 100개의 결정 트리 사용\n",
        "- 결과를 모아 다수결 투표나 평균으로 최종 결과 결정\n",
        "- 참고 사이트 : https://tinyurl.com/3aawud7s\n",
        "- 참고 사이트 : https://tinyurl.com/58vn6mj9\n",
        "- 참고 사이트 : https://tinyurl.com/yad46zrc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://www.tibco.com/sites/tibco/files/media_entity/2021-05/random-forest-diagram.svg)"
      ],
      "metadata": {
        "id": "jCZQcma-EC9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 부트스트랩 샘플 bootstrap sample\n",
        "---\n",
        "- 데이터 세트에서 중복을 허용하여 데이터를 샘플링 하는 방식\n",
        "- 기본적으로 부트스틀랩 샘플은 훈련세트의 크기와 같게 만듬\n",
        "- 부트스트랩 샘플을 가지고 결정트리 생성 \n",
        "- 그 이후 배깅 Bagging 과 부스팅 Boosting  방법을 이용 랜덤 포레스트 Random Forest가 만들어 짐\n",
        "- 나무를 모으는 방법이 배깅과 부스팅"
      ],
      "metadata": {
        "id": "W5TcZnMeBhBM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://t1.daumcdn.net/cfile/tistory/993C9C505A92751109)"
      ],
      "metadata": {
        "id": "BOTERYDxHBhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 배깅 Bagging \n",
        "---\n",
        "- **B**ootstrap **Agg**regat**ing** 준말\n",
        "- 부트스트랩 Bootstrap 샘플링을 이용하여 주어진 하나의 데이터로 학습된 예측 모델 보다 더 좋은 모델을 만들 수 있는 앙상블 기법\n",
        "- 샘플을 여러 번 뽑아(Bootstrap) 각 모델을 학습시켜 결과물을 집계(Aggregration)하는 방법\n",
        "- Categorical Data는 투표 방식(Votinig)으로 결과를 집계하며, Continuous Data는 평균으로 집계"
      ],
      "metadata": {
        "id": "RUJ1-_ETOiX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Bagging](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb4wG8O%2FbtqyfYW98AS%2FYZBtUJy3jZLyuik1R0aGNk%2Fimg.png)"
      ],
      "metadata": {
        "id": "TNiJK_i5SRyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 부스팅 Boosting\n",
        "---\n",
        "- 부스팅은 가중치를 활용하여 약 분류기를 강 분류기로 만드는 방법\n",
        "- 배깅이 독립적인 결정 트리가 각각 값을 예측한 뒤, 그 결과 값을 집계해 최종 결과 값을 예측하는 방식이라면 부스팅은 모델 간 팀워크\n",
        "- 처음 모델이 예측을 하면 그 예측 결과에 따라 데이터에 가중치가 부여\n",
        "- 분류가 잘못된 데이터에 가중치를 부여해주는 이유는 다음 모델에서 더 집중해 분류하기 위함\n",
        "- 부여된 가중치가 다음 모델에 영향을 줌\n",
        "- 잘못 분류된 데이터에 집중하여 새로운 분류 규칙을 만드는 단계를 반복\n",
        "- 참고 사이트 : https://tinyurl.com/yad46zrc\n"
      ],
      "metadata": {
        "id": "SwMiMgc6SYph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**부스팅 Boosting : + -로 구성된 데이터 셋 분류 문제**"
      ],
      "metadata": {
        "id": "aV6WbfP_UyKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Boosting](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FkCejr%2FbtqyghvqEZB%2F9o3rKTEsuSIDHEfelYFJlk%2Fimg.png)"
      ],
      "metadata": {
        "id": "XkesZYSmUk5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**D1 Part**\n",
        "-  2/5 지점을 횡단\n",
        "- 위쪽의 +는 잘못 분류가 되었고, 아래쪽의 두 -도 잘못 분류\n",
        "- 잘못 분류가 된 데이터는 가중치를 높여주고, 잘 분류된 데이터는 가중치를 낮추어 줌 "
      ],
      "metadata": {
        "id": "tq4I8LS_Uu-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**D2 Part**\n",
        "-  D1에서 잘 분류된 데이터는 크기 작아 짐 => 가중치 낮아짐\n",
        "- 위쪽의 +는 잘못 분류가 되었고, 아래쪽의 두 -도 잘못 분류\n",
        "- 잘못 분류된 +는 커짐 => 가중치 커짐\n",
        "- D2에서는 오른쪽 세 개의 -가 잘못 분류"
      ],
      "metadata": {
        "id": "nhqK3OdNYKzW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**D3 Part**\n",
        "- 세 개의 -의 가중치가 커졌음\n",
        "- 맨 처음 모델에서 가중치를 부여한 +와 -는 D2에서는 잘 분류가 되었기 때문에 D3에서는 가중치가 다시 작아짐"
      ],
      "metadata": {
        "id": "omuV7dV8V8Gt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combined classifier**\n",
        "- D1, D2, D3의 Classifier를 합쳐 최종 Classifier를 구할 수 있습니다. 최종 Classfier는 +와 -를 정확하게 구분"
      ],
      "metadata": {
        "id": "JLh7zgcSaG7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 의사 결정 트리와 랜덤 포레스트의 차이점\n",
        "---\n",
        "- 랜덤 포레스트는 의사 결정 트리의 그룹\n",
        "- 의사 결정 트리는 의사 결정에 사용하는 규칙을 만드는 경향\n",
        "- 랜덤 포레스트는 기능을 무작위로 선택하고 관찰하여 의사 결정 트리의 포리스트를 만든 다음 결과를 평균화\n",
        "- 이론에 따르면 많은 수의 상관되지 않은 트리가 하나의 개별 의사 결정 트리보다 더 정확한 예측을 생성\n",
        "- 이는 많은 트리들이 함께 작동하여 개별 오류와 과적합으로부터 서로를 보호하기 때문\n"
      ],
      "metadata": {
        "id": "o7emSCveIKUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 랜덤 포레스트 알고리즘은 비즈니스에서 어떻게 사용됩니까?\n",
        "---\n",
        "> 랜덤 포레스트는 비즈니스 환경에서 많이 응용됩니다. 예를 들어, 단일 의사 결정 트리에서는 와인과 관련된 데이터 세트를 분류하여 다양한 와인을 농도가 연한 와인 또는 농도가 진한 와인으로 구분할 수 있습니다.\n",
        "\n",
        "> 랜덤 포레스트는 많은 트리를 생성하므로 최종 결과 예측을 훨씬 더 정교하게 합니다. 가격, 탄닌, 산도, 알코올 함량, 설탕, 가용성 및 다양한 기타 모든 기능을 비교하여 와인과 여러 트리를 가질 수 있습니다. 그런 다음 결과를 평균화하여 수많은 기준을 기반으로 하여 전반적으로 (거의 틀림없이) 최고의 와인을 예측할 수 있습니다.\n",
        "\n",
        "> 비즈니스에서 랜덤 포레스트 알고리즘은 입력 데이터 범위가 다양하고 상황이 복잡한 시나리오에서 사용할 수 있습니다. 고객이 회사를 떠날 시기를 식별하는 것을 예로 들겠습니다. 고객 이탈은 복잡하며 일반적으로 제품 비용, 최종 제품에 대한 만족도, 고객 지원 효율성, 지불 용이성, 계약 기간, 제공되는 추가 기능, 성별, 연령, 위치 등 많은 요소를 포함합니다. 랜덤 포레스트 알고리즘은 이러한 모든 요소에 대한 의사 결정 트리를 생성하여 조직의 고객 중 이탈 위험이 높은 고객을 정확하게 예측할 수 있습니다.\n",
        "\n",
        "> 또 다른 복잡한 예는 어떤 고객이 한 해에 가장 많은 지출을 할 것인지 예측하는 것입니다. 마케팅 부서는 종합적인 변수와 속성을 분석하여 그 해에 누구를 목표로 삼아야 하는지에 대해 예측을 할 수 있습니다.\n",
        "\n",
        "참고 사이트 : https://www.tibco.com/ko/reference-center/what-is-a-random-forest"
      ],
      "metadata": {
        "id": "F9G4XONyJDqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 배깅과 부스팅 차이 \n",
        "---\n",
        "> 배깅\n",
        "> 병렬로 학습\n",
        "> \n",
        "\n",
        "> 부스팅\n",
        "> 순차적으로 학습\n",
        "> 한번 학습이 끝난 후 결과에 따라 가중치를 부여\n",
        "> 오답에 대해서는 높은 가중치를 부여\n",
        "> 오답을 정답으로 맞추기 위해 오답에 더 집중\n",
        "> 속도가 느리고 오버 피팅이 될 가능성 높음\n",
        "\n",
        "> 개별 결정 트리의 낮은 성능이 문제라면 부스팅이 적합하고, 오버 피팅이 문제라면 배깅이 적합"
      ],
      "metadata": {
        "id": "ILhEBxTQaVwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![배깅과 부스팅 차이](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbwr6JW%2FbtqygiHRbRk%2Fcy5hbDAPpTjCG7xa6UWxi0%2Fimg.png)"
      ],
      "metadata": {
        "id": "vQSbLyHtai4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "uclqAfzm8y8A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ioJUlZ0M_uSZ"
      },
      "outputs": [],
      "source": [
        "# 데이터 준비\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "wine = pd.read_csv('https://bit.ly/wine_csv_data')\n",
        "\n",
        "data = wine[['alcohol', 'sugar', 'pH']].to_numpy()\n",
        "target = wine['class'].to_numpy()\n",
        "\n",
        "train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDKQudr7_8nu",
        "outputId": "57e2bb66-ac53-455c-bdf5-7a2acc407cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9973541965122431 0.8905151032797809\n"
          ]
        }
      ],
      "source": [
        "# 교차 검증 수행, 기본 100개 결정 트리\n",
        "# return_train_score=True : 검증 점수 뿐만 아니라 훈련 세트에 대한 점수도 반환\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
        "scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 과대적합\n",
        "- RandomForestClassifier클래스는 교차 검증 수행 시 기본 100개 결정 트리\n",
        "- return_train_score=True : 검증 점수 뿐만 아니라 훈련 세트에 대한 점수도 반환"
      ],
      "metadata": {
        "id": "SR4zGNbE9f_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf2 = RandomForestClassifier(n_jobs=-1, random_state=42, max_depth=5)\n",
        "scores = cross_validate(rf2, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhUI2lRu_uvI",
        "outputId": "c08e3414-43d3-41d2-a4b4-3c6e577c59a2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8696363615536095 0.8608834308136523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYDbzXNLG8fK",
        "outputId": "faa88231-e5a7-4537-d7c1-515292983042"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.23167441 0.50039841 0.26792718]\n"
          ]
        }
      ],
      "source": [
        "# 특성 중요도 확인 \n",
        "rf.fit(train_input, train_target)\n",
        "print(rf.feature_importances_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 결정 트리 모델의 특성 중요도 : [ 0.12345626 0.86862934 0.0079144 ]\n",
        "- 당도의 중요도는 감소하고 알코올 도수와 pH특성의 중요도 상승\n",
        "- 랜덤 포레스트가 특성의 일부를 랜덤하게 선택하여 결정 트리를 훈련하기 때문\n",
        "- 랜덤 포레스트는 과대적합을 줄이고 일반화 성능을 높이는 데 도움이 됨"
      ],
      "metadata": {
        "id": "SldMoHWP-Sm4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMc06S1Fa_A-",
        "outputId": "f1afcde9-d067-4925-fda0-593d39970dc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8934000384837406\n"
          ]
        }
      ],
      "source": [
        "# oob_score Out Of Bag\n",
        "rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)\n",
        "\n",
        "rf.fit(train_input, train_target)\n",
        "print(rf.oob_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- OOB Out Of Bag 부트스트랩 샘플에 포함되지 않고 남은 샘플\n",
        "- oob_score=True OOB로 평가, 검증 세트 처럼 사용 가능\n",
        "- 교차 검증의 테스트 세트와 비슷한 결과"
      ],
      "metadata": {
        "id": "tebinHhD_bLg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdrVoeQZRU14"
      },
      "source": [
        "## 엑스트라 트리 Extra Tree\n",
        "---\n",
        "- 결정트리가 제공하는 대분의 매개변수 제공\n",
        "- 부트스트랩 샘플을 사용 하지 않고 전체 훈련 세트 사용\n",
        "- 대신 노드를 분할 할때 가장 좋은 분할을 찾는 것이 아니라 무작위 분할\n",
        "- DecisionTreeClassifier의 splitter='random' 결정 트리 사용\n",
        "- 무작위 분활이라 성능이 낮아지겠지만 많은 트리를 앙상블하기 때문에 과대적합을 막고 검증 세트의 점수를 높이는 효과\n",
        "- 특성이 많으면 최적의 분할을 찾는 것은 시간이 많이 걸림\n",
        "- 무작위 분할은 특성이 많을 때 빠른 트리 구성이 가능\n",
        "- 보통 엑스트라 트리가 더 많은 결정 트리를 훈련 해야함, 기본 100개 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noMLdywdOGrE",
        "outputId": "bf244b97-1abc-4f16-8807-fe20bac2779e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9974503966084433 0.8887848893166506\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "et = ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
        "scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 특성이 많지 않아 랜덤포레스트와 비슷한 결과 나옴 "
      ],
      "metadata": {
        "id": "_GB4db6eCKTW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnB0_mBqfcXL",
        "outputId": "fac3d7d7-93ea-47f2-ba1b-78d01f443578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.20183568 0.52242907 0.27573525]\n"
          ]
        }
      ],
      "source": [
        "# 특성 중요도 확인\n",
        "et.fit(train_input, train_target)\n",
        "print(et.feature_importances_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csKxnaxeRX8s"
      },
      "source": [
        "## 그레이디언트 부스팅 Gradient boosting\n",
        "---\n",
        "- 깊이가 얕은 결정 트리 사용하여 오차를 보완하는 방식의 앙상블 방법\n",
        "- 사이킷런의 GradientBoostingClassifier 클래스는 기본적으로 깊이 3 결정트리 100개 사용\n",
        "- 깊이가 얕아 과대 적합에 강하고 일반화 성능 기대\n",
        "- 그레이디언트 경사 하강법을 사용하여 트리를 앙상블에 추가\n",
        "- 분류에서는 로지스틱 손실 함수, 회구에서는 평균 제곱 오차를 사용\n",
        "- 학습율 매개변수로 속도 조절"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IlNEFkaNsoG",
        "outputId": "3bf6ad7b-dd97-455e-9edd-e579c57656d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8881086892152563 0.8720430147331015\n"
          ]
        }
      ],
      "source": [
        "# GradientBoostingClassifier로 교차 검증\n",
        "# 기본 깊이 3, 결정 트리 100개, 학습률 0.1\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNpeS8EWpeEi",
        "outputId": "a6f240a6-c4c1-4584-f382-ec5bacb682ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9464595437171814 0.8780082549788999\n"
          ]
        }
      ],
      "source": [
        "# 앙상블에 추가될 결정 트리 갯수와 학습률 늘려서 확인\n",
        "gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)\n",
        "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 트리의 갯수를 늘려도 과대적합에 강함"
      ],
      "metadata": {
        "id": "TrRxjdBOFLkd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD6iWVsGqCAE",
        "outputId": "6bd3a2dc-ac93-46dc-e5ce-233844355e42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.15872278 0.68010884 0.16116839]\n"
          ]
        }
      ],
      "source": [
        "# 특성 중요도 확인 `feature_importances_`\n",
        "gb.fit(train_input, train_target)\n",
        "print(gb.feature_importances_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 랜덤포레스트 보다 일부 특성 당도에 더 집중된 모습 "
      ],
      "metadata": {
        "id": "SfNEyN1tFx8u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BthW_II9RbLa"
      },
      "source": [
        "## 히스토그램 기반 부스팅 Histogram-based Gredient Boosting\n",
        "---\n",
        "- 정형 데이터를 다루는 머신러닝 알고리즘 중 인기가 높은 알고리즘\n",
        "- 그레이디언트 부스팅의 속도와 성능을 개선한 것\n",
        "- 사이킷런 HistGradientBoosting\n",
        "- 그외 XGBoost,LightGBM 이 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 사이킷런 HistGradientBoostingClassifier\n",
        "---\n",
        "- 사이킷런 버전에 주의\n",
        "-  특성 중요도를 계산하기 위해서 permutation_importance() 메소드를 사용\n",
        "- n_repeats는 랜덤하게 섞을 횟수를 의미, 기본값은 5\n",
        "- importances_mean에 접근하면 특성 중요도의 평균"
      ],
      "metadata": {
        "id": "sgTAVpuQWhvA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3Ct_NNWQbdA",
        "outputId": "4eee87eb-9adf-4f2d-8ce3-65d69069c10e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9321723946453317 0.8801241948619236\n"
          ]
        }
      ],
      "source": [
        "# 사이킷런 1.0 버전 아래에서는 다음 라인의 주석을 해제하고 실행하세요.\n",
        "# from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "hgb = HistGradientBoostingClassifier(random_state=42)\n",
        "scores = cross_validate(hgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 결과적으로 수치를 보면 과대 적합을 잘 억제하면서 그레이디언트 부스팅보다 높은 효과"
      ],
      "metadata": {
        "id": "Risw_ur7Xxag"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvlB0GMTS3hn",
        "outputId": "b1fde447-9a6a-408f-eabd-a448f0793fa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.08876275 0.23438522 0.08027708]\n"
          ]
        }
      ],
      "source": [
        "# 특성 중요도 확인\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "hgb.fit(train_input, train_target)\n",
        "result = permutation_importance(hgb, train_input, train_target, n_repeats=10,\n",
        "                                random_state=42, n_jobs=-1)\n",
        "print(result.importances_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 이 함수는 특성을 하나씩 랜덤하게 섞어서 모델의 성능이 변화하는지를 관찰하고, 어떤 특성이 중요한지 계산\n",
        "- 매개변수 n_repeats는 랜덤하게 섞을 횟수를 의미하는데, 기본값은 5\n",
        "- importances_mean에 접근하면 특성 중요도의 평균갑 "
      ],
      "metadata": {
        "id": "npUnggmBYJSP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8FfxInn-xBQ",
        "outputId": "eaa11cf3-c7c3-427e-a39f-a1eb81b5003f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.05969231 0.20238462 0.049     ]\n"
          ]
        }
      ],
      "source": [
        "# 테스트 세트에서 특성 중요도 확인\n",
        "result = permutation_importance(hgb, test_input, test_target, n_repeats=10,\n",
        "                                random_state=42, n_jobs=-1)\n",
        "print(result.importances_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 결과를 보면 그레이디언트 부스팅과 비슷하게 당도에 조금 더 집중\n",
        "- 이런 분석을 통해 모델이 실전에서 어떤 특성에 관심을 둘지 예상해볼 수 있음"
      ],
      "metadata": {
        "id": "QZV2FwwbeO9v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqplZjh0j2nw",
        "outputId": "c6200ec8-4acd-4745-a5c6-2cb43477b64f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8723076923076923"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# 최종 성능 확인\n",
        "hgb.score(test_input, test_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fz_FrezBezR"
      },
      "source": [
        "### XGBoost\n",
        "---\n",
        "- 대표적인 그레이디언트 부스팅 알고리즘\n",
        "- 사이킷런에서 제공하는 클래스 아님에도 불구하고 cross_validate() 메소드와 함께 사용 가능\n",
        "- tree_method를 'hist'로 지정하면 히스토그램 기반 그레이디언트 부스팅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBYLvOiV6rga",
        "outputId": "efc8e5df-f4ec-45cf-8c95-1ac652fbade0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8824322471423747 0.8726214185237284\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb = XGBClassifier(tree_method='hist', random_state=42)\n",
        "scores = cross_validate(xgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl6nh6DOBd-B"
      },
      "source": [
        "### LightGBM\n",
        "---\n",
        "- 마이크로소프트에서 만든 라이브러리로, 사이킷런의 히스토그램 기반 그레이디언트 부스팅에 큰 영향을 줌\n",
        "-  사이킷런에서 제공하는 cross_validate() 메소드와 함께 사용 가능\n",
        "- XGBoost를 사용했을 때보다는 약간의 과대 적합한 경향이 있지만 다른 라이브러리에 비해 잘 억제하고 있음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maihlDMP7lmY",
        "outputId": "46482ea8-afd8-46fd-ba99-e92c3d6f0448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9338079582727165 0.8789710890649293\n"
          ]
        }
      ],
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "lgb = LGBMClassifier(random_state=42)\n",
        "scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "default:Python",
      "language": "python",
      "name": "conda-env-default-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}